from sqlalchemy.orm import sessionmaker, scoped_session
from database.database import engine
from database.models import Finding
from bs4 import BeautifulSoup
import scrapy
import urllib.parse
import logging

logger = logging.getLogger(__name__)


class VulnAnalysisPipeline:
    
    # --- Payloads y Firmas para Detección ---
    SQLI_PAYLOADS = ["' OR 1=1 --", "' OR 'a'='a", '" OR "a"="a']
    SQLI_ERROR_STRINGS = [
        "you have an error in your sql syntax",
        "warning: mysql",
        "unclosed quotation mark",
        "sql command not properly ended"
    ]
    XSS_PAYLOAD = "<script>alert('XSS-VULN-TAG')</script>"

    def __init__(self):
        self.Session = scoped_session(sessionmaker(bind=engine))

    def open_spider(self, spider):
        """Inicializa recursos al abrir el spider."""
        self.crawler = spider.crawler
        logger.info("VulnAnalysisPipeline iniciado")

    def close_spider(self, spider):
        """Limpia recursos al cerrar el spider."""
        try:
            self.Session.remove()
            logger.info("VulnAnalysisPipeline cerrado correctamente")
        except Exception as e:
            logger.error(f"Error cerrando pipeline: {e}")

    def process_item(self, item, spider):
        """
        Procesa el item realizando checks pasivos.
        Los checks activos se programan como requests separados.
        """
        try:
            # --- Checks pasivos (análisis de la respuesta original) ---
            self.check_missing_security_headers(item)
            self.check_software_version_leak(item)
            self.check_form_without_csrf_token(item)

            # --- Programar checks activos ---
            # Estos se ejecutan como requests adicionales del crawler
            self._schedule_active_checks(item, spider)
            
        except Exception as e:
            logger.error(f"Error procesando item {item.get('url', 'unknown')}: {e}", exc_info=True)
        
        return item

    def _schedule_active_checks(self, item, spider):
        """
        Programa los checks activos como nuevos requests en el crawler.
        """
        try:
            # Genera requests para SQLi
            for request in self.check_sql_injection(item):
                spider.crawler.engine.crawl(request)
            
            # Genera requests para XSS
            for request in self.check_xss(item):
                spider.crawler.engine.crawl(request)
                
        except Exception as e:
            logger.error(f"Error programando checks activos para {item.get('url')}: {e}")

    def save_finding(self, url, http_status, vuln_type, severity, details):
        """Guarda un hallazgo en la base de datos de forma thread-safe."""
        session = self.Session()
        try:
            # Verifica si ya existe
            exists = session.query(Finding).filter_by(
                url=url, 
                vulnerability_type=vuln_type
            ).first()
            
            if exists:
                logger.debug(f"Hallazgo duplicado ignorado: {vuln_type} en {url}")
                return

            finding = Finding(
                url=url,
                http_status=http_status,
                vulnerability_type=vuln_type,
                severity=severity,
                details=details
            )
            session.add(finding)
            session.commit()
            logger.info(f"✅ Hallazgo guardado: {vuln_type} en {url}")
            
        except Exception as e:
            session.rollback()
            logger.error(f"Error guardando hallazgo: {e}", exc_info=True)
        finally:
            session.close()

    # --- Métodos de Checks Pasivos ---
    
    def check_missing_security_headers(self, item):
        """Verifica la presencia de cabeceras de seguridad importantes."""
        headers_to_check = {
            'X-Frame-Options': 'Baja',
            'Content-Security-Policy': 'Media',
            'Strict-Transport-Security': 'Baja',
            'X-Content-Type-Options': 'Baja'
        }
        
        try:
            for header, severity in headers_to_check.items():
                # Verifica tanto en bytes como en string
                header_bytes = header.encode('utf-8')
                if not (item['response_headers'].get(header_bytes) or 
                       item['response_headers'].get(header)):
                    details = f"La cabecera de seguridad '{header}' no está presente."
                    self.save_finding(
                        item['url'], 
                        item['response_status'], 
                        'Cabecera de Seguridad Faltante', 
                        severity, 
                        details
                    )
        except Exception as e:
            logger.error(f"Error en check_missing_security_headers: {e}")

    def check_software_version_leak(self, item):
        """Detecta posibles fugas de versión de software."""
        try:
            body = item['response_body'].decode('utf-8', errors='ignore').lower()
            
            # Expande la detección a más tecnologías
            version_patterns = {
                "powered by wordpress": "WordPress",
                "x-powered-by: php": "PHP",
                "server: apache": "Apache",
                "drupal": "Drupal",
                "joomla": "Joomla"
            }
            
            for pattern, software in version_patterns.items():
                if pattern in body:
                    details = f"Se ha detectado una posible versión de {software} expuesta."
                    self.save_finding(
                        item['url'], 
                        item['response_status'], 
                        'Fuga de Versión de Software', 
                        'Baja', 
                        details
                    )
                    break  # Solo reporta la primera coincidencia
                    
        except Exception as e:
            logger.error(f"Error en check_software_version_leak: {e}")

    def check_form_without_csrf_token(self, item):
        """Verifica si los formularios tienen protección CSRF."""
        try:
            soup = BeautifulSoup(item['response_body'], 'html.parser')
            
            for form in soup.find_all('form'):
                # Busca tokens CSRF en inputs ocultos
                has_csrf = False
                for inp in form.find_all('input', {'type': 'hidden'}):
                    name_attr = inp.get('name')
                    name = str(name_attr).lower() if name_attr else ""
                    if 'csrf' in name or 'token' in name or '_token' in name:
                        has_csrf = True
                        break
                
                if not has_csrf:
                    action = form.get('action', 'N/A')
                    details = f"Formulario sin token CSRF visible. Acción: {action}"
                    self.save_finding(
                        item['url'], 
                        item['response_status'], 
                        'Formulario sin Token CSRF', 
                        'Media', 
                        details
                    )
        except Exception as e:
            logger.error(f"Error en check_form_without_csrf_token: {e}")

    # --- Checks Activos ---
    
    def check_sql_injection(self, item):
        """
        Genera requests con payloads de SQLi para detectar vulnerabilidades.
        Retorna generador de scrapy.Request.
        """
        try:
            soup = BeautifulSoup(item['response_body'], 'html.parser')
            
            for form in soup.find_all('form'):
                action = urllib.parse.urljoin(item['url'], str(form.get('action', '')))
                method = str(form.get('method', 'GET')).upper()
                
                # Extrae campos del formulario
                inputs = form.find_all('input')
                if not inputs:
                    continue
                
                for payload in self.SQLI_PAYLOADS:
                    data = {}
                    for inp in inputs:
                        name = inp.get('name')
                        if name:
                            # Inyecta payload en campos de texto
                            inp_type_attr = inp.get('type', 'text')
                            inp_type = str(inp_type_attr).lower() if inp_type_attr else "text"
                            if inp_type in ['text', 'search', 'email']:
                                data[str(name)] = payload
                            else:
                                data[str(name)] = inp.get('value', '')
                    
                    if not data:
                        continue
                    
                    # Crea el request apropiado
                    if method == 'POST':
                        yield scrapy.FormRequest(
                            action, 
                            formdata=data, 
                            callback=self.parse_sqli_response,
                            cb_kwargs={'payload': payload, 'original_url': item['url']},
                            errback=self.handle_request_error,
                            dont_filter=True
                        )
                    else:
                        url = f"{action}?{urllib.parse.urlencode(data)}"
                        yield scrapy.Request(
                            url, 
                            callback=self.parse_sqli_response,
                            cb_kwargs={'payload': payload, 'original_url': item['url']},
                            errback=self.handle_request_error,
                            dont_filter=True
                        )
        except Exception as e:
            logger.error(f"Error en check_sql_injection: {e}")

    def parse_sqli_response(self, response, payload, original_url):
        """Analiza la respuesta de un intento de SQLi."""
        try:
            body = response.text.lower()
            
            for error in self.SQLI_ERROR_STRINGS:
                if error in body:
                    details = (
                        f"Error SQL detectado: '{error}' "
                        f"al inyectar payload: '{payload}'. "
                        f"Formulario en: {original_url}"
                    )
                    self.save_finding(
                        response.url, 
                        response.status, 
                        'Inyección de SQL (SQLi)', 
                        'Alta', 
                        details
                    )
                    break  # Reporta el primer error encontrado
                    
        except Exception as e:
            logger.error(f"Error en parse_sqli_response: {e}")

    def check_xss(self, item):
        """
        Genera requests con payloads de XSS para detectar vulnerabilidades.
        Retorna generador de scrapy.Request.
        """
        try:
            soup = BeautifulSoup(item['response_body'], 'html.parser')
            
            for form in soup.find_all('form'):
                action = urllib.parse.urljoin(item['url'], str(form.get('action', '')))
                method = str(form.get('method', 'GET')).upper()
                
                # Solo inyecta en campos de entrada visibles
                data = {}
                for inp in form.find_all('input'):
                    name = inp.get('name')
                    inp_type_attr = inp.get('type', 'text')
                    inp_type = str(inp_type_attr).lower() if inp_type_attr else "text"

                    if name and inp_type not in ['hidden', 'submit', 'button']:
                        data[str(name)] = self.XSS_PAYLOAD
                
                if not data:
                    continue

                # Crea el request apropiado
                if method == 'POST':
                    yield scrapy.FormRequest(
                        action, 
                        formdata=data, 
                        callback=self.parse_xss_response,
                        cb_kwargs={'original_url': item['url']},
                        errback=self.handle_request_error,
                        dont_filter=True
                    )
                else:
                    url = f"{action}?{urllib.parse.urlencode(data)}"
                    yield scrapy.Request(
                        url, 
                        callback=self.parse_xss_response,
                        cb_kwargs={'original_url': item['url']},
                        errback=self.handle_request_error,
                        dont_filter=True
                    )
        except Exception as e:
            logger.error(f"Error en check_xss: {e}")

    def parse_xss_response(self, response, original_url):
        """Analiza si el payload de XSS se reflejó en la respuesta."""
        try:
            if self.XSS_PAYLOAD in response.text:
                details = (
                    f"Payload de script reflejado sin escapar. "
                    f"Formulario en: {original_url}"
                )
                self.save_finding(
                    response.url, 
                    response.status, 
                    'Cross-Site Scripting (XSS) Reflejado', 
                    'Alta', 
                    details
                )
        except Exception as e:
            logger.error(f"Error en parse_xss_response: {e}")

    def handle_request_error(self, failure):
        """Maneja errores en requests de checks activos."""
        logger.error(f"Error en request de check activo: {failure}")
